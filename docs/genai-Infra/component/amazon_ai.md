---
sidebar_position: 1
---
# Overview of compute Platform for generative AI in AWS

* Amazon Elastic Compute Cloud (Amazon EC2) accelerated computing portfolio [including instances powered by GPUs and purpose-built ML silicon](https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing) offers the broadest choice of accelerators to power generative AI workloads.

* Coupled with other managed services such as [Amazon SageMaker HyperPod](https://aws.amazon.com/sagemaker/hyperpod/) and Amazon Elastic Kubernetes Service [Amazon EKS](https://aws.amazon.com/eks/), these instances provide developers with the industryâ€™s best platform for building and deploying generative AI applications. [Gen AI on EKS](https://awslabs.github.io/data-on-eks/docs/gen-ai)

* Amazon Elastic Compute Cloud (EC2) Trn1 instances, powered by [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/) accelerators, are purpose built for high-performance deep learning (DL) training of generative AI models, including large language models (LLMs) and latent diffusion models

* [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/) accelerators are designed by AWS to deliver high performance at the lowest cost in Amazon EC2 for your deep learning (DL) and generative AI inference applications. 

* [AWS Neuron SDK](https://aws.amazon.com/machine-learning/neuron/) helps developers deploy models on the AWS Inferentia accelerators (and train them on AWS Trainium accelerators). It integrates natively with popular frameworks, such as PyTorch and TensorFlow, so that you can continue to use your existing code and workflows and run on Inferentia accelerators.
